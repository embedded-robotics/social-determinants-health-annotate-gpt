{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84664ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import openai\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import signal\n",
    "import tiktoken\n",
    "from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0873fb",
   "metadata": {},
   "source": [
    "## Extract Data into Accepted Format\n",
    "### Unnanotated DataFrame Necessary Columns:\n",
    "- <b>ID</b> Unique identifier number for ease of reference\n",
    "- <b>TEXT</b> This column should hold all relevant text that should be annotated by AnnotateGPT\n",
    "\n",
    "##### Example Unnanotated Sample\n",
    "<table>\n",
    "  <tr>\n",
    "    <th><center>ROW_ID</center></th>\n",
    "    <th><center>TEXT</center></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><center>1</center></td>\n",
    "    <td><center>Patient...</center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><center>...</center></td>\n",
    "    <td><center>...</center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><center>2038</center></td>\n",
    "    <td><center>The patient comes from...</center></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "### Unnanotated DataFrame Necessary Columns:\n",
    "- <b>ID</b> Unique identifier number for ease of reference\n",
    "- <b>TEXT</b> This column should hold all relevant text that should be annotated by AnnotateGPT\n",
    "- <b>LABEL NAME</b> This column should hold the categorization for the label in question. There may be more than one <i>LABEL NAME</i> column, and each should have its own unique name.\n",
    "\n",
    "##### Example Annotated Sample\n",
    "<table>\n",
    "  <tr>\n",
    "    <th><center>ROW_ID</center></th>\n",
    "    <th><center>TEXT</center></th>\n",
    "    <th><center>sdoh_community_present</center></th>\n",
    "    <th><center>sdoh_economics</center></th>\n",
    "    <th><center>behavior_tobacco</center></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><center>1</center></th>\n",
    "    <td><center>The patient...</center></td>\n",
    "    <td><center>1</center></td>\n",
    "    <td><center>0</center></td>\n",
    "    <td><center>1</center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><center>...</center></td>\n",
    "    <td><center>...</center></td>\n",
    "    <td><center>...</center></td>\n",
    "    <td><center>...</center></td>\n",
    "    <td><center>...</center></td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td><center>233</center></td>\n",
    "    <td><center>Patient's family...</center></td>\n",
    "    <td><center>1</center></td>\n",
    "    <td><center>0</center></td>\n",
    "    <td><center>0</center></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "### Example Extraction of MIMIC-III and MIMIC-SBDH Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIQUE_ID_COLUMN_NAME = \"ROW_ID\"\n",
    "UNIQUE_TEXT_COLUMN_NAME = \"TEXT\"\n",
    "UNIQUE_LABEL_COLUMN_NAMES = ['sdoh_community_present','sdoh_economics','behavior_tobacco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c48b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_social_history(df):\n",
    "    replace_texts = []\n",
    "    for row_id in df[UNIQUE_ID_COLUMN_NAME]:\n",
    "        patient = df[df[UNIQUE_ID_COLUMN_NAME] == row_id][UNIQUE_TEXT_COLUMN_NAME].iloc[0]\n",
    "        social_history_start = patient.lower().find('social history:')\n",
    "        pos_ends = []\n",
    "        pos_ends.append(patient.lower().find('family history:'))\n",
    "        pos_ends.append(patient.lower().find('physical exam'))\n",
    "        pos_ends.append(patient.lower().find('medications:'))\n",
    "        pos_ends.append(patient.lower().find('hospital course:'))\n",
    "        pos_ends.append(patient.lower().find('review of systems:'))\n",
    "        pos_ends = [x for x in pos_ends if x > social_history_start]\n",
    "        pos_ends.append(social_history_start+500)\n",
    "        social_history_end = min(pos_ends)\n",
    "        replace_texts.append((row_id,patient[social_history_start:social_history_end]))\n",
    "    texts = pd.DataFrame(replace_texts,columns =[UNIQUE_ID_COLUMN_NAME,UNIQUE_TEXT_COLUMN_NAME])\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa7e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths to MIMIC_CSVs\n",
    "MIMIC_ADMISSION_CSV = \"ADMISSIONS.csv\" #Fill in path/to/file with the path to your MIMIC-III folder\n",
    "MIMIC_NOTEEVENTS_CSV = \"NOTEEVENTS.csv\" #Fill in path/to/file with the path to your MIMIC-III folder\n",
    "MIMIC_SBDH = \"MIMIC-SBDH.csv\" #Fill in path/to/file with the path to your MIMIC-SBDH folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading DataFrames for Annotated and Unnanotated MIMIC Notes\n",
    "\n",
    "df = pd.read_csv(MIMIC_ADMISSION_CSV)\n",
    "newborn_list = df[df[\"ADMISSION_TYPE\"] == \"NEWBORN\"].SUBJECT_ID.to_list()\n",
    "notes_df = pd.read_csv(MIMIC_NOTEEVENTS_CSV)\n",
    "discharge_df = notes_df[notes_df['CATEGORY'] == 'Discharge summary']\n",
    "non_neonatal = discharge_df[~discharge_df['SUBJECT_ID'].isin(newborn_list)]\n",
    "sbdh_data = pd.read_csv(open(MIMIC_SBDH, 'r+', encoding='UTF-8'),encoding='UTF-8', on_bad_lines='warn')\n",
    "sbdh_data = sbdh_data.rename(columns={'row_id':UNIQUE_ID_COLUMN_NAME})\n",
    "annotated_list = sbdh_data[UNIQUE_ID_COLUMN_NAME].tolist()\n",
    "annotated_notes = discharge_df[discharge_df[UNIQUE_ID_COLUMN_NAME].isin(annotated_list)]\n",
    "annotated_subjects = discharge_df[discharge_df[UNIQUE_ID_COLUMN_NAME].isin(annotated_list)].SUBJECT_ID.to_list()\n",
    "\n",
    "no_soc_his = []\n",
    "for index, row in non_neonatal.iterrows():\n",
    "    if 'social history:' not in row[UNIQUE_TEXT_COLUMN_NAME].lower():\n",
    "        no_soc_his.append(row[UNIQUE_ID_COLUMN_NAME])\n",
    "\n",
    "final_sdoh_list = non_neonatal[~non_neonatal[UNIQUE_ID_COLUMN_NAME].isin(no_soc_his)]\n",
    "unnanotated_notes = final_sdoh_list[~final_sdoh_list[UNIQUE_ID_COLUMN_NAME].isin(annotated_list)]\n",
    "\n",
    "annotated_sh = retrieve_social_history(annotated_notes)\n",
    "annotated_sh = pd.merge(annotated_sh,sbdh_data[[UNIQUE_ID_COLUMN_NAME] + UNIQUE_LABEL_COLUMN_NAMES],on=UNIQUE_ID_COLUMN_NAME, how='left')\n",
    "unannotated_sh = retrieve_social_history(unnanotated_notes)\n",
    "\n",
    "df = newborn_list = notes_df = discharge_df = non_neonatal = annotated_list = annotated_subjects = no_soc_his = final_sdoh_list = unnanotated = sbdh_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5894ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "economics_binary = [1 if x == 2 else 0 for x in annotated_sh.sdoh_economics.to_list()]\n",
    "tobacco_binary = [1 if x == 1 or x == 2 else 0 for x in annotated_sh.behavior_tobacco.to_list()]\n",
    "annotated_sh = annotated_sh.drop(columns=['sdoh_economics','behavior_tobacco'])\n",
    "annotated_sh['sdoh_economics'] = economics_binary\n",
    "annotated_sh['behavior_tobacco'] = tobacco_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a736aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30349ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unannotated_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f0949",
   "metadata": {},
   "source": [
    "## Setting up API access to Microsoft Azure OpenAI Instance\n",
    "Setup is dependent on your version of the API, as well as the nature of your instance. The example below uses V0.28 of the openai python module, and makes use of the Completion endpoint. Instructions on how to fill in the API access information below can be found in these cookbook examples: (https://github.com/Azure/openai-samples/blob/main/Basic_Samples/Chat/chatGPT_managing_conversation.ipynb) and (https://github.com/Azure/openai-samples/blob/main/Basic_Samples/Chat/config.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffdc38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block is for API access. Replace it with whatever API you use.\n",
    "    \n",
    "chatgpt_model_name = None\n",
    "openai.api_type = None\n",
    "openai.api_key = None\n",
    "openai.api_base = None\n",
    "openai.api_version = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb79e9b",
   "metadata": {},
   "source": [
    "## Setting up prompt messages\n",
    "There are 3 kinds of prompt messages that must be set:\n",
    "- <b>Instructions</b> - The instructions are as follows: a succinct roleplaying instruction designed to contextualize the GPT model; a General Task Instruction that explains the kind of information, which is important to focus on for the task; SDoH Specific Instruction that explicitly states which kinds of information must be extracted.\n",
    "- <b>Query</b> - The query is a Yes/No question pertaining to the task\n",
    "- <b>Examples</b> - The examples are constructed using pairs of \"Shots\" â€“ exemplar responses presented alongside the prompt to assist in guiding GPT. These Shots are consistently presented with the positive shot before the negative shot within Examples section. This approach is systematically employed to craft four unique Two-Shot prompts, which are explained in detail in the Methods section of our paper\n",
    "\n",
    "## Example prompt setup for MIMIC tasks\n",
    "Select one of the 3 MIMIC tasks available. Change the variable associated with the task to True for the desired task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a MIMIC task. Only one must be true, two must be false.\n",
    "# Default: community\n",
    "community = True\n",
    "economics = False\n",
    "tobacco = False\n",
    "\n",
    "assert community + economics + tobacco == 1, \"One and only one must be True, the other two must be False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if community:\n",
    "    task = 'community'\n",
    "    label_column = \"sdoh_community_present\"\n",
    "elif economics:\n",
    "    task = 'economics'\n",
    "    label_column = \"sdoh_economics\"\n",
    "else:\n",
    "    task = 'tobacco'\n",
    "    label_column = \"behavior_tobacco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9febdb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompts = pickle.load(open('MIMIC_TASK_PROMPTS.pkl','rb'))\n",
    "\n",
    "base_system_message = task_prompts[task]['instructions']\n",
    "system_message = f\"<|im_start|>INSTRUCTIONS:\\n{base_system_message.strip()}\\n<|im_end|>\"\n",
    "query_message = task_prompts[task]['query']\n",
    "\n",
    "easy_example_pos = annotated_sh[annotated_sh[UNIQUE_ID_COLUMN_NAME] == task_prompts[task]['examples']['easy_example_pos']].iloc[0].TEXT.replace('\\n', ' ').strip()\n",
    "easy_answer_pos = task_prompts[task]['examples']['easy_answer_pos']\n",
    "easy_answer_pos_explained = task_prompts[task]['examples']['easy_answer_pos_explained']\n",
    "easy_example_neg = annotated_sh[annotated_sh[UNIQUE_ID_COLUMN_NAME] == task_prompts[task]['examples']['easy_example_neg']].iloc[0].TEXT.replace('\\n', ' ').strip()\n",
    "easy_answer_neg = task_prompts[task]['examples']['easy_answer_neg']\n",
    "easy_answer_neg_explained = task_prompts[task]['examples']['easy_answer_neg_explained']\n",
    "\n",
    "hard_example_pos = annotated_sh[annotated_sh[UNIQUE_ID_COLUMN_NAME] == task_prompts[task]['examples']['hard_example_pos']].iloc[0].TEXT.replace('\\n', ' ').strip()\n",
    "hard_answer_pos = task_prompts[task]['examples']['hard_answer_pos']\n",
    "hard_answer_pos_explained = task_prompts[task]['examples']['hard_answer_pos_explained']\n",
    "hard_example_neg = annotated_sh[annotated_sh[UNIQUE_ID_COLUMN_NAME] == task_prompts[task]['examples']['hard_example_neg']].iloc[0].TEXT.replace('\\n', ' ').strip()\n",
    "hard_answer_neg = task_prompts[task]['examples']['hard_answer_neg']\n",
    "hard_answer_neg_explained = task_prompts[task]['examples']['hard_answer_neg_explained']\n",
    "\n",
    "example_ids = [task_prompts[task]['examples']['easy_example_pos'],task_prompts[task]['examples']['easy_example_neg'],task_prompts[task]['examples']['hard_example_pos'],task_prompts[task]['examples']['hard_example_neg']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c7e9e",
   "metadata": {},
   "source": [
    "## Prepare funtions\n",
    "Make sure to run these cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ddfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to create the prompt from the instruction system message, the few-shot examples, and the current query\n",
    "# The function assumes 'examples' is a list of few-shot examples in dictionaries with 'context', 'query' and 'answer' keys\n",
    "# Example: examples = [{\"context\": \"Lives with wife, no tobacco, no alcohol, no drugs\",\n",
    "# \"query\": \"Does the social history present tobacco use?\", \"answer\": \"No.\"}]\n",
    "# The function assumes 'query' is a dictionary containing the current query GPT is expected to answer with 'context' and 'query' keys.\n",
    "# Example: query = [{\"context\": \"Lives alone, history of 1 ppd, no alcohol use, no drug use\", \n",
    "# \"query\": \"Does the social history present tobacco use?\"}]\n",
    "def create_prompt(system_message, examples, query):\n",
    "    prompt = system_message\n",
    "    if examples != None:\n",
    "        for example in examples:\n",
    "            prompt += f\"\\n<|im_start|>CONTEXT:\\n{example['context']}\\n<|im_end|>\"\n",
    "            prompt += f\"\\n<|im_start|>QUERY:\\n{example['query']}\\n<|im_end|>\"\n",
    "            prompt += f\"\\n<|im_start|>ANSWER:\\n{example['answer']}\\n<|im_end|>\"\n",
    "    prompt += f\"\\n<|im_start|>CONTEXT:\\n{query['context']}\\n<|im_end|>\"\n",
    "    prompt += f\"\\n<|im_start|>QUERY:\\n{query['query']}\\n<|im_end|>\"\n",
    "    prompt += f\"\\n<|im_start|>ANSWER:\\n\"\n",
    "    return prompt\n",
    "\n",
    "# This function sends the prompt to the GPT model\n",
    "def send_message(prompt, model_name, max_response_tokens=500):\n",
    "    response = openai.Completion.create(\n",
    "        engine=chatgpt_model_name,\n",
    "        prompt=prompt,\n",
    "        temperature=0.5,\n",
    "        max_tokens=max_response_tokens,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=['<|im_end|>']\n",
    "    )\n",
    "    return response['choices'][0]['text'].strip()\n",
    "\n",
    "# timeout handler\n",
    "def alarm_handler(signum, frame):\n",
    "    print(\"Timeout... Retrying.\")\n",
    "    raise Exception()\n",
    "\n",
    "# Defining a function to estimate the number of tokens in a prompt\n",
    "def estimate_tokens(prompt):\n",
    "    cl100k_base = tiktoken.get_encoding(\"cl100k_base\") \n",
    "\n",
    "    enc = tiktoken.Encoding( \n",
    "        name=\"chatgpt\",  \n",
    "        pat_str=cl100k_base._pat_str, \n",
    "        mergeable_ranks=cl100k_base._mergeable_ranks, \n",
    "        special_tokens={ \n",
    "            **cl100k_base._special_tokens, \n",
    "            \"<|im_start|>\": 100264, \n",
    "            \"<|im_end|>\": 100265\n",
    "        } \n",
    "    ) \n",
    "\n",
    "    tokens = enc.encode(prompt,  allowed_special={\"<|im_start|>\", \"<|im_end|>\"})\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bfbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_examples(shots, example_hard, example_explained):\n",
    "    if shots:\n",
    "        if example_hard:\n",
    "            context_messages = [hard_example_pos, hard_example_neg]\n",
    "            if example_explained:\n",
    "                answer_messages = [hard_answer_pos_explained, hard_answer_neg_explained]\n",
    "            else:\n",
    "                answer_messages = [hard_answer_pos, hard_answer_neg]\n",
    "        else:\n",
    "            context_messages = [easy_example_pos, easy_example_neg]\n",
    "            if example_explained:\n",
    "                answer_messages = [easy_answer_pos_explained, easy_answer_neg_explained]\n",
    "            else:\n",
    "                answer_messages = [easy_answer_pos, easy_answer_neg]  \n",
    "\n",
    "        examples = [{\"context\": context_messages[0], \"query\": query_message, \"answer\": answer_messages[0]},{\"context\": context_messages[1], \"query\": query_message, \"answer\": answer_messages[1]}]\n",
    "\n",
    "    else:\n",
    "        examples = None\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209aa125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block sends the test set one by one and gathers responses from GPT\n",
    "# I tried to paralelize it, but it kept throwing timeout errors from the GPT side, not sure what I was doing wrong\n",
    "# you can try, if you're up for it. its linear right now and that doesn't even get close to the rate limit\n",
    "\n",
    "\n",
    "def gpt_annotation(dataframe, examples, num_annotations):\n",
    "    responses = []\n",
    "    tokens = []\n",
    "\n",
    "    GPT_positives = 0\n",
    "    GPT_negatives = 0\n",
    "\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        sys.stdout.write(\"\\r\")\n",
    "        sys.stdout.write(\"{} examples remaining.\".format((num_annotations)-GPT_positives-GPT_negatives))\n",
    "        sys.stdout.flush()\n",
    "        context_query = row[UNIQUE_TEXT_COLUMN_NAME]\n",
    "\n",
    "        signal.signal(signal.SIGALRM, alarm_handler)\n",
    "        context_query = context_query.strip()\n",
    "        \n",
    "        query = {\"context\": context_query, \"query\": query_message}\n",
    "        \n",
    "        prompt = create_prompt(system_message, examples, query)\n",
    "        \n",
    "        for attempt in range(4):\n",
    "            try:\n",
    "                max_response_tokens = 500\n",
    "\n",
    "                signal.alarm(5)\n",
    "                response = send_message(prompt, chatgpt_model_name, max_response_tokens)\n",
    "                signal.alarm(0)\n",
    "\n",
    "                if 'Yes' in response and GPT_positives < num_annotations/2:\n",
    "                    responses.append((row[UNIQUE_ID_COLUMN_NAME],1))\n",
    "                    GPT_positives += 1\n",
    "                elif 'No' in response and GPT_negatives < num_annotations/2:\n",
    "                    responses.append((row[UNIQUE_ID_COLUMN_NAME],0))\n",
    "                    GPT_negatives += 1\n",
    "            except Exception as error:\n",
    "                print(\"An exception occurred:\", error)\n",
    "                continue\n",
    "            break\n",
    "        if GPT_positives >= num_annotations/2 and GPT_negatives >= num_annotations/2:\n",
    "            sys.stdout.write(\"\\r\")\n",
    "            sys.stdout.write(\"{} examples remaining.\".format((num_annotations)-GPT_positives-GPT_negatives))\n",
    "            sys.stdout.flush()\n",
    "            break\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c7916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block sends the test set one by one and gathers responses from GPT\n",
    "# I tried to paralelize it, but it kept throwing timeout errors from the GPT side, not sure what I was doing wrong\n",
    "# you can try, if you're up for it. its linear right now and that doesn't even get close to the rate limit\n",
    "\n",
    "\n",
    "def gpt_test_annotation(dataframe, examples, num_annotations):\n",
    "    responses = []\n",
    "    tokens = []\n",
    "\n",
    "    GPT_annotations = 0\n",
    "\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        sys.stdout.write(\"\\r\")\n",
    "        sys.stdout.write(\"{} examples remaining.\".format((num_annotations)-GPT_annotations))\n",
    "        sys.stdout.flush()\n",
    "        context_query = row[UNIQUE_TEXT_COLUMN_NAME]\n",
    "        label = row[label_column]\n",
    "\n",
    "        signal.signal(signal.SIGALRM, alarm_handler)\n",
    "        context_query = context_query.strip()\n",
    "        \n",
    "        query = {\"context\": context_query, \"query\": query_message}\n",
    "        \n",
    "        prompt = create_prompt(system_message, examples, query)\n",
    "        \n",
    "        for attempt in range(4):\n",
    "            try:\n",
    "                max_response_tokens = 500\n",
    "\n",
    "                signal.alarm(5)\n",
    "                response = send_message(prompt, chatgpt_model_name, max_response_tokens)\n",
    "                signal.alarm(0)\n",
    "\n",
    "                if 'Yes' in response:\n",
    "                    responses.append((row[UNIQUE_ID_COLUMN_NAME],1,label))\n",
    "                    GPT_annotations += 1\n",
    "                elif 'No' in response:\n",
    "                    responses.append((row[UNIQUE_ID_COLUMN_NAME],0,label))\n",
    "                    GPT_annotations += 1\n",
    "            except Exception as error:\n",
    "                print(\"An exception occurred:\", error)\n",
    "                continue\n",
    "            break\n",
    "        if GPT_annotations >= num_annotations:\n",
    "            sys.stdout.write(\"\\r\")\n",
    "            sys.stdout.write(\"{} examples remaining.\".format((num_annotations)-GPT_annotations))\n",
    "            sys.stdout.flush()\n",
    "            break\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f8d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_best_threshold(predictions, true_labels):\n",
    "    \n",
    "    class_preds = [1 if (x > 0.5) else 0 for x in predictions]\n",
    "    cm = confusion_matrix(true_labels, class_preds)\n",
    "    target_names = ['negative', 'positive']\n",
    "    clss_report = classification_report(true_labels, class_preds, target_names=target_names,digits=4)\n",
    "    \n",
    "    return {'clss_report':clss_report, 'confusion_matrix':cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1269d88",
   "metadata": {},
   "source": [
    "## Annotating MIMIC task training sets with AnnotateGPT\n",
    "This section performs annotation of unnanotated samples with AnnotateGPT for the 3 MIMIC tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The total number of annotations that will be performed by AnnotateGPT\n",
    "# Default: 2048\n",
    "num_annotations = 2048\n",
    "\n",
    "assert num_annotations%2 == 0, \"num_annotations must be even\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e15e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shots = True\n",
    "example_hard = [False,True]\n",
    "example_explained = [False,True]\n",
    "arr1 = np.array([[False, False, False]])\n",
    "arr2 = np.array(np.meshgrid(shots, example_hard, example_explained)).T.reshape(-1,3)\n",
    "all_annotation_types = np.concatenate((arr1,arr2))\n",
    "all_annotation_names = ['ZeroShot','TwoShot-E','TwoShot-H','TwoShot-E+Ex','TwoShot-H+Ex']\n",
    "\n",
    "for idx, ann_name in enumerate(all_annotation_names):\n",
    "\n",
    "    examples = prepare_examples(all_annotation_types[idx][0], all_annotation_types[idx][1], all_annotation_types[idx][2])\n",
    "\n",
    "    print('Annotating with',ann_name)\n",
    "    responses = gpt_annotation(unannotated_sh, examples, num_annotations)\n",
    "    print('\\nAnnotation Complete.')\n",
    "\n",
    "    resp_df = pd.DataFrame(responses, columns=[UNIQUE_ID_COLUMN_NAME, label_column])\n",
    "\n",
    "    file_name = f\"{ann_name}-{task}-gpt-train.pkl\"\n",
    "    pickle.dump(resp_df,open(file_name, 'wb'))\n",
    "    print(\"{} training set saved to {}\".format(ann_name, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c892aefd",
   "metadata": {},
   "source": [
    "## Calculating Inter-Annotator Agreement for MIMIC tasks\n",
    "In this section, we use AnnotateGPT to annotate a set of human annotated samples in order to calculate the Cohen's Kappa between the human annotation and the AnnotateGPT annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b48422",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_id_lists = pickle.load(open('train-test-id-lists.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e43217",
   "metadata": {},
   "outputs": [],
   "source": [
    "if community:\n",
    "    task = 'com'\n",
    "    label_column = \"sdoh_community_present\"\n",
    "elif economics:\n",
    "    task = 'eco'\n",
    "    label_column = \"sdoh_economics\"\n",
    "else:\n",
    "    task = 'tob'\n",
    "    label_column = \"behavior_tobacco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd05633",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = train_test_id_lists[f'{task}_test_list']\n",
    "test_sh = annotated_sh[annotated_sh[UNIQUE_ID_COLUMN_NAME].isin(test_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14481822",
   "metadata": {},
   "outputs": [],
   "source": [
    "shots = True\n",
    "example_hard = [False,True]\n",
    "example_explained = [False,True]\n",
    "arr1 = np.array([[False, False, False]])\n",
    "arr2 = np.array(np.meshgrid(shots, example_hard, example_explained)).T.reshape(-1,3)\n",
    "all_annotation_types = np.concatenate((arr1,arr2))\n",
    "all_annotation_names = ['ZeroShot','TwoShot-E','TwoShot-H','TwoShot-E+Ex','TwoShot-H+Ex']\n",
    "\n",
    "for idx, ann_name in enumerate(all_annotation_names):\n",
    "\n",
    "    examples = prepare_examples(all_annotation_types[idx][0], all_annotation_types[idx][1], all_annotation_types[idx][2])\n",
    "\n",
    "    print('Annotating with',ann_name)\n",
    "    responses = gpt_test_annotation(test_sh, examples, len(test_sh))\n",
    "    print('\\nAnnotation Complete.')\n",
    "\n",
    "    resp_df = pd.DataFrame(responses, columns=[UNIQUE_ID_COLUMN_NAME, f'{label_column}_GPT',label_column])\n",
    "\n",
    "    file_name = f\"{ann_name}-{task}-cohen_calc_set.pkl\"\n",
    "    pickle.dump(resp_df,open(file_name, 'wb'))\n",
    "    print(\"{} training set saved to {}\".format(ann_name, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0f4e1",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9724410",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ann_name in all_annotation_names:\n",
    "    print(f'----------------------{ann_name}-----------------------')\n",
    "    resp_df = pickle.load(open(f'{ann_name}-{task}-cohen_calc_set.pkl','rb'))\n",
    "    gold = resp_df.sdoh_community_present.to_list()\n",
    "    annotategpt = resp_df.sdoh_community_present_GPT.to_list()\n",
    "    metrics = get_metrics_best_threshold(annotategpt, gold)\n",
    "    print(metrics['clss_report'])\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "    print(\"Cohen's kappa:\",cohen_kappa_score(annotategpt, gold))\n",
    "    print(f'------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e5e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
